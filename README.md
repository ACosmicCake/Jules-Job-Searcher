# AI Job Agent

## Overview
A Python-based AI agent designed to assist in automating various parts of the job search process. It helps manage job applications by parsing CVs, storing user preferences, scraping job listings, and (eventually) interacting with job platforms.

## Features (Planned & Current)
*   Parses CVs from DOCX and PDF formats (`cv_parser.py`).
*   Manages user's personal information and job search preferences via `config.json`.
*   Stores structured data extracted from CVs into `parsed_cv_data.json`.
*   **Scrapes job listings from multiple platforms (Indeed, LinkedIn, etc.) using `python-jobspy`**.
*   **Stores scraped job listings in an SQLite database (`job_listings.db`), handling duplicates.**
*   Loads configuration and parsed CV data for use by the main agent (`agent.py`).
*   (Future) Tailors job applications using parsed CV data and cover letter templates.
*   (Future) Interacts with job boards or APIs to find and apply for jobs.
*   (Future) Matches parsed CV content against job descriptions.

## Project Structure
Here's a brief overview of the key files and directories in this project:

*   `config.json`: Stores your personal information, job preferences, and paths to your CV files. **You must update this file.**
*   `cv_parser.py`: Python script responsible for reading CV files (DOCX and PDF) and extracting structured information.
*   `parsed_cv_data.json`: The JSON file where the structured data extracted from your CV will be stored. This file is generated by `cv_parser.py`.
*   `agent.py`: The main Python script that runs the AI agent, utilizing the configuration and parsed CV data, and can trigger job scraping.
*   `scraper.py`: **Module responsible for fetching job listings using `python-jobspy` and storing them in the database.**
*   `database_setup.py`: **Script to initialize the SQLite database (`job_listings.db`) and `jobs` table.**
*   `job_listings.db`: **SQLite database storing scraped job data. (Note: Should typically be gitignored. Schema is set up by `database_setup.py`).**
*   `dummy_cv.docx`, `dummy_cv.pdf`: Example CV files provided for testing the parsing functionality.
*   `generate_dummy_docx.py`: Utility script to generate `dummy_cv.docx`.
*   `generate_dummy_pdf.py`: Utility script to generate `dummy_cv.pdf` using `reportlab`.
*   `tests.py`: Outlines the testing strategy for the project.
*   `README.md`: This file, providing information about the project.

## Setup Instructions

### 1. Prerequisites
*   Python 3.7 or newer.
*   Git (for cloning).

### 2. Clone the Repository
```bash
git clone https://github.com/your-username/ai-job-agent.git # Replace with actual URL
cd ai-job-agent
```

### 3. Create and Activate a Virtual Environment
It's highly recommended to use a virtual environment to manage project dependencies.
```bash
python -m venv venv
```
Activate the virtual environment:
*   On macOS and Linux:
    ```bash
    source venv/bin/activate
    ```
*   On Windows:
    ```bash
    venv\Scripts\activate
    ```

### 4. Install Dependencies
Install the necessary Python libraries using pip:
```bash
pip install python-docx pdfplumber reportlab python-jobspy pandas
```
*   `python-docx`: For reading text from .docx files.
*   `pdfplumber`: For extracting text from .pdf files.
*   `reportlab`: Used by `generate_dummy_pdf.py` (optional if not re-generating dummy PDF).
*   `python-jobspy`: For scraping job listings from various platforms.
*   `pandas`: A core dependency for `python-jobspy` and data manipulation.

### 5. Configure `config.json`
Open the `config.json` file and update it with your:
*   Personal information (name, email, phone, LinkedIn, etc.).
*   Job preferences (desired roles, locations, salary, etc.).
*   **Crucially, update `cv_paths`** to point to the actual location of your CV files.

### 6. Initialize the Database
This step creates the `job_listings.db` file and the necessary `jobs` table for storing scraped job data.
```bash
python database_setup.py
```

### 7. Place Your CVs (Optional, for CV Parsing Features)
Ensure your actual CV files are accessible by the paths you specified in `config.json` if you intend to use the CV parsing features.

## Usage

### 1. Prepare Your Data (CV Parsing - Optional)
If you want the agent to use information from your CV:
*   Ensure `cv_parser.py` is configured (if necessary) and your CV paths in `config.json` are correct.
*   Run the CV parser:
    ```bash
    python cv_parser.py
    ```
    This will create/update `parsed_cv_data.json`.
    *(Note: The `cv_parser.py` is still under development. Full integration with `config.json` for dynamic CV path loading and ensuring `parsed_cv_data.json` is the primary output will be refined.)*

### 2. Run Job Search Scraping
This step uses `python-jobspy` to find jobs based on criteria in `config.json` (currently, the first role and location) and stores them in `job_listings.db`. The agent triggers this process.
To run the agent, which includes the job search:
```bash
python agent.py
```
The `scraper.py` module handles the fetching and storing. You can also run `python scraper.py` directly if you only want to perform the scraping and storing step without other agent functionalities.

### 3. Using the Agent (Further Capabilities)
Running `python agent.py` currently initializes the agent, loads configurations, and triggers the job search (Step 2).
Future capabilities will include:
*   Matching your parsed CV data against scraped job descriptions.
*   Assisting with cover letter generation.
*   Tracking application statuses.

## CV Parsing Details (Briefly)
The `cv_parser.py` script is designed to extract text from your CV and identify key sections (Skills, Work Experience, Education, etc.). CV parsing is complex; adjustments to `cv_parser.py` might be needed for different CV formats.

## Database Schema
The `job_listings.db` SQLite database contains a `jobs` table with the following key columns:
*   `id` (TEXT, PRIMARY KEY): Stores the unique job URL.
*   `title` (TEXT, NOT NULL): Job title.
*   `company` (TEXT, NOT NULL): Company name.
*   `location` (TEXT): Job location.
*   `date_posted` (TEXT): Date the job was posted.
*   `description_text` (TEXT, NOT NULL): Full job description.
*   `job_url` (TEXT, NOT NULL, UNIQUE): The source URL of the job posting (same as `id`).
*   `application_url` (TEXT): Direct URL to the application page, if available.
*   `source` (TEXT, NOT NULL): Platform where the job was found (e.g., 'indeed', 'linkedin').
*   `scraped_timestamp` (TEXT, NOT NULL): When the job was scraped.
*   `status` (TEXT, NOT NULL, DEFAULT 'new'): Tracks application status (e.g., 'new', 'applied', 'ignored').

## Contributing
Contributions are welcome! If you'd like to improve the agent or add new features:
1.  Fork the repository.
2.  Create a new branch for your feature (`git checkout -b feature/AmazingFeature`).
3.  Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4.  Push to the branch (`git push origin feature/AmazingFeature`).
5.  Open a Pull Request.

---

*This README provides a general guide. Functionality and specific commands may evolve as the project develops.*
